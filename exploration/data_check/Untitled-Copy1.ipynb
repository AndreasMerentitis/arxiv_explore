{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "target_name_dict = { 'astro-ph.GA' : 0,\n",
    "                    'astro-ph.SR' : 1,\n",
    "                    'astro-ph.IM' : 2,\n",
    "                    'astro-ph.EP' : 3,\n",
    "                    'astro-ph.HE' : 4,\n",
    "                    'astro-ph.CO' : 5\n",
    "                }\n",
    "label2target = { v:k for k,v in target_name_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.HDFStore(\"../data/2014astroph_p.h5\", \"r\")\n",
    "df['/df'].keys()\n",
    "abstracts = df['/df']['abstract']\n",
    "labels = np.array(df['/df']['label'])\n",
    "df.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2256 astro-ph.GA\n",
      "We show that the mass fraction of GMC gas (n>100 cm^-3) in dense (n>>10^4\n",
      "cm^-3) star-forming clumps, observable in dense molecular tracers\n",
      "(L_HCN/L_CO(1-0)), is a sensitive probe of the strength and mechanism(s) of\n",
      "stellar feedback. Using high-resolution galaxy-scale simulations with pc-scale\n",
      "resolution and explicit models for feedback from radiation pressure,\n",
      "photoionization heating, stellar winds, and supernovae (SNe), we make\n",
      "predictions for the dense molecular gas tracers as a function of GMC and galaxy\n",
      "properties and the efficiency of stellar feedback. In models with weak/no\n",
      "feedback, much of the mass in GMCs collapses into dense sub-units, predicting\n",
      "L_HCN/L_CO(1-0) ratios order-of-magnitude larger than observed. By contrast,\n",
      "models with feedback properties taken directly from stellar evolution\n",
      "calculations predict dense gas tracers in good agreement with observations.\n",
      "Changing the strength or timing of SNe tends to move systems along, rather than\n",
      "off, the L_HCN-L_CO relation (because SNe heat lower-density material, not the\n",
      "high-density gas). Changing the strength of radiation pressure (which acts\n",
      "efficiently in the highest density gas), however, has a much stronger effect on\n",
      "L_HCN than on L_CO. We predict that the fraction of dense gas (L_HCN/L_CO(1-0))\n",
      "increases with increasing GMC surface density; this drives a trend in\n",
      "L_HCN/L_CO(1-0) with SFR and luminosity which has tentatively been observed.\n",
      "Our results make specific predictions for enhancements in the dense gas tracers\n",
      "in unusually dense environments such as ULIRGs and galactic nuclei (including\n",
      "the galactic center).\n"
     ]
    }
   ],
   "source": [
    "j = np.random.randint(len(labels))\n",
    "print(j, label2target[labels[j]])\n",
    "print(abstracts[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "maxlen = 150\n",
    "training_samples = 2000\n",
    "validation_samples = 10000\n",
    "max_words = 10000 # Top 10000 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30677 unique tokens\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(abstracts)\n",
    "sequences = tokenizer.texts_to_sequences(abstracts)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print(\"Found %s unique tokens\" % len(word_index))\n",
    "\n",
    "word_index_reverse = dict()\n",
    "\n",
    "for k, v in word_index.items():\n",
    "    word_index_reverse[v] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pad_sequences(sequences=sequences, maxlen=maxlen)\n",
    "indices = np.arange(abstracts.shape[0])\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "data = data[indices]\n",
    "\n",
    "labels = labels[indices]\n",
    "\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "\n",
    "x_val = data[training_samples:training_samples+validation_samples]\n",
    "y_val = labels[training_samples:training_samples+validation_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/41971587/how-to-convert-predicted-sequence-back-to-text-in-keras\n",
    "# Creating a reverse dictionary\n",
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "\n",
    "# Function takes a tokenized sentence and returns the words\n",
    "def sequence_to_text(list_of_indices):\n",
    "    # Looking up words in dictionary\n",
    "    words = [reverse_word_map.get(letter) for letter in list_of_indices]\n",
    "    return(words)\n",
    "\n",
    "# Creating texts \n",
    "my_texts = list(map(sequence_to_text, data))\n",
    "\n",
    "# alternative way\n",
    "my_texts_2 = tokenizer.sequences_to_texts(sequences=sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "740 astro-ph.CO\n",
      "[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 'uncertainty', 'in', 'the', 'calibration', 'of', 'gravitational', 'wave', 'gw', 'detector', 'data', 'leads', 'to', 'systematic', 'errors', 'which', 'must', 'be', 'accounted', 'for', 'in', 'setting', 'limits', 'on', 'the', 'strength', 'of', 'gw', 'signals', 'when', 'cross', 'correlation', 'measurements', 'are', 'made', 'using', 'data', 'from', 'a', 'pair', 'of', 'instruments', 'as', 'in', 'searches', 'for', 'a', 'stochastic', 'gw', 'background', 'the', 'calibration', 'uncertainties', 'of', 'the', 'individual', 'instruments', 'can', 'be', 'combined', 'into', 'an', 'uncertainty', 'associated', 'with', 'the', 'pair', 'with', 'the', 'advent', 'of', 'multi', 'baseline', 'gw', 'observation', 'e', 'g', 'networks', 'consisting', 'of', 'multiple', 'detectors', 'such', 'as', 'the', 'ligo', 'observatories', 'and', 'virgo', 'a', 'more', 'sophisticated', 'treatment', 'is', 'called', 'for', 'we', 'describe', 'how', 'the', 'correlations', 'between', 'calibration', 'factors', 'associated', 'with', 'different', 'pairs', 'can', 'be', 'taken', 'into', 'account', 'by', 'marginalizing', 'over', 'the', 'uncertainty', 'associated', 'with', 'each', 'instrument']\n"
     ]
    }
   ],
   "source": [
    "j = np.random.randint(len(x_train))\n",
    "print(j, label2target[y_train[j]])\n",
    "print(my_texts[indices[j]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "y_train_one_hot = to_categorical(y_train)\n",
    "y_val_one_hot = to_categorical(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing GloVe file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "glove_dir = \"../glove.6B/\"\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dim = 100 # same dimension as the glove.6B above\n",
    "embedding_vector = None\n",
    "embedding_matrix = np.zeros((max_words, embeddings_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "import keras.layers as layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 150, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 15000)             0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 64)                960064    \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 1,964,614\n",
      "Trainable params: 1,964,614\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Embedding(max_words, embeddings_dim, input_length=maxlen))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(maxlen,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(6, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable= False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 6794 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 1s 360us/step - loss: 1.9488 - acc: 0.2680 - val_loss: 1.7706 - val_acc: 0.3182\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 127us/step - loss: 1.5871 - acc: 0.3910 - val_loss: 1.7383 - val_acc: 0.3243\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 127us/step - loss: 1.2220 - acc: 0.5505 - val_loss: 2.3683 - val_acc: 0.3382\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 121us/step - loss: 0.8085 - acc: 0.7285 - val_loss: 2.0520 - val_acc: 0.2754\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 124us/step - loss: 0.4949 - acc: 0.8400 - val_loss: 2.2873 - val_acc: 0.2440\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 122us/step - loss: 0.2551 - acc: 0.9345 - val_loss: 2.6601 - val_acc: 0.2053\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 123us/step - loss: 0.1044 - acc: 0.9750 - val_loss: 3.1811 - val_acc: 0.2817\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 127us/step - loss: 0.0811 - acc: 0.9780 - val_loss: 3.1535 - val_acc: 0.2551\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 135us/step - loss: 0.0681 - acc: 0.9825 - val_loss: 3.2276 - val_acc: 0.2608\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 123us/step - loss: 0.0917 - acc: 0.9800 - val_loss: 3.4289 - val_acc: 0.2583\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train, y_train_one_hot,\n",
    "                   epochs=10,\n",
    "                   batch_size=32,\n",
    "                   validation_data = (x_val, y_val_one_hot))\n",
    "#model.save_weights('pre_trained_glove_model_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_text = tokenizer.sequences_to_texts(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'accurate estimate of the turbulent energy spectrum we then apply this method to the 13co map of ngc 1333 from the complete database we find the turbulent energy spectrum is a power law e k k beta in the range of scales 0 06 pc ell 1 5 pc with slope beta 1 85 pm 0 04 the estimated energy injection scale of stellar outflows in ngc 1333 is ell 0 3 pc well resolved by the observations there is no evidence of the flattening of the energy spectrum above the scale ell predicted by outflow driven simulations and analytical models the power spectrum of integrated intensity is also a nearly perfect power law in the range of scales 0 16 pc ell 7 9 pc with no feature above ell we conclude that the observed turbulence in ngc 1333 does not appear to be driven primarily by stellar outflows'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accurate',\n",
       " 'estimate',\n",
       " 'of',\n",
       " 'the',\n",
       " 'turbulent',\n",
       " 'energy',\n",
       " 'spectrum',\n",
       " 'we',\n",
       " 'then',\n",
       " 'apply',\n",
       " 'this',\n",
       " 'method',\n",
       " 'to',\n",
       " 'the',\n",
       " '13co',\n",
       " 'map',\n",
       " 'of',\n",
       " 'ngc',\n",
       " '1333',\n",
       " 'from',\n",
       " 'the',\n",
       " 'complete',\n",
       " 'database',\n",
       " 'we',\n",
       " 'find',\n",
       " 'the',\n",
       " 'turbulent',\n",
       " 'energy',\n",
       " 'spectrum',\n",
       " 'is',\n",
       " 'a',\n",
       " 'power',\n",
       " 'law',\n",
       " 'e',\n",
       " 'k',\n",
       " 'k',\n",
       " 'beta',\n",
       " 'in',\n",
       " 'the',\n",
       " 'range',\n",
       " 'of',\n",
       " 'scales',\n",
       " '0',\n",
       " '06',\n",
       " 'pc',\n",
       " 'ell',\n",
       " '1',\n",
       " '5',\n",
       " 'pc',\n",
       " 'with',\n",
       " 'slope',\n",
       " 'beta',\n",
       " '1',\n",
       " '85',\n",
       " 'pm',\n",
       " '0',\n",
       " '04',\n",
       " 'the',\n",
       " 'estimated',\n",
       " 'energy',\n",
       " 'injection',\n",
       " 'scale',\n",
       " 'of',\n",
       " 'stellar',\n",
       " 'outflows',\n",
       " 'in',\n",
       " 'ngc',\n",
       " '1333',\n",
       " 'is',\n",
       " 'ell',\n",
       " '0',\n",
       " '3',\n",
       " 'pc',\n",
       " 'well',\n",
       " 'resolved',\n",
       " 'by',\n",
       " 'the',\n",
       " 'observations',\n",
       " 'there',\n",
       " 'is',\n",
       " 'no',\n",
       " 'evidence',\n",
       " 'of',\n",
       " 'the',\n",
       " 'flattening',\n",
       " 'of',\n",
       " 'the',\n",
       " 'energy',\n",
       " 'spectrum',\n",
       " 'above',\n",
       " 'the',\n",
       " 'scale',\n",
       " 'ell',\n",
       " 'predicted',\n",
       " 'by',\n",
       " 'outflow',\n",
       " 'driven',\n",
       " 'simulations',\n",
       " 'and',\n",
       " 'analytical',\n",
       " 'models',\n",
       " 'the',\n",
       " 'power',\n",
       " 'spectrum',\n",
       " 'of',\n",
       " 'integrated',\n",
       " 'intensity',\n",
       " 'is',\n",
       " 'also',\n",
       " 'a',\n",
       " 'nearly',\n",
       " 'perfect',\n",
       " 'power',\n",
       " 'law',\n",
       " 'in',\n",
       " 'the',\n",
       " 'range',\n",
       " 'of',\n",
       " 'scales',\n",
       " '0',\n",
       " '16',\n",
       " 'pc',\n",
       " 'ell',\n",
       " '7',\n",
       " '9',\n",
       " 'pc',\n",
       " 'with',\n",
       " 'no',\n",
       " 'feature',\n",
       " 'above',\n",
       " 'ell',\n",
       " 'we',\n",
       " 'conclude',\n",
       " 'that',\n",
       " 'the',\n",
       " 'observed',\n",
       " 'turbulence',\n",
       " 'in',\n",
       " 'ngc',\n",
       " '1333',\n",
       " 'does',\n",
       " 'not',\n",
       " 'appear',\n",
       " 'to',\n",
       " 'be',\n",
       " 'driven',\n",
       " 'primarily',\n",
       " 'by',\n",
       " 'stellar',\n",
       " 'outflows']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu-2",
   "language": "python",
   "name": "tf-gpu-2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
