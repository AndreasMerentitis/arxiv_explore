{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "target_name_dict = { 'astro-ph.GA' : 0,\n",
    "                    'astro-ph.SR' : 1,\n",
    "                    'astro-ph.IM' : 2,\n",
    "                    'astro-ph.EP' : 3,\n",
    "                    'astro-ph.HE' : 4,\n",
    "                    'astro-ph.CO' : 5\n",
    "                }\n",
    "\n",
    "label2target = { v:k for k,v in target_name_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/local/EC/andreas.merentitis/Sagemaker/Mygit/arxiv_explore/exploration/data_check\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.HDFStore(\"../../data/2015astroph.h5\", \"r\")\n",
    "df['/df'].keys()\n",
    "abstracts = df['/df']['abstract']\n",
    "labels = np.array(df['/df']['categories'])\n",
    "df.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44745 0\n",
      "Gravitationally lensed (GL) quasars are brighter than their unlensed\n",
      "counterparts and produce images with distinctive morphological signatures. Past\n",
      "searches and target selection algorithms, in particular the Sloan Quasar Lens\n",
      "Search (SQLS), have relied on basic morphological criteria, which were applied\n",
      "to samples of bright, spectroscopically confirmed quasars. The SQLS techniques\n",
      "are not sufficient for searching into new surveys (e.g. DES, PS1, LSST),\n",
      "because spectroscopic information is not readily available and the large data\n",
      "volume requires higher purity in target/candidate selection. We carry out a\n",
      "systematic exploration of machine learning techniques and demonstrate that a\n",
      "two step strategy can be highly effective. In the first step we use\n",
      "catalog-level information ($griz$+WISE magnitudes, second moments) to preselect\n",
      "targets, using artificial neural networks. The accepted targets are then\n",
      "inspected with pixel-by-pixel pattern recognition algorithms (Gradient-Boosted\n",
      "Trees), to form a final set of candidates. The results from this procedure can\n",
      "be used to further refine the simpler SQLS algorithms, with a twofold (or\n",
      "threefold) gain in purity and the same (or $80\\%$) completeness at\n",
      "target-selection stage, or a purity of $70\\%$ and a completeness of $60\\%$\n",
      "after the candidate-selection step. Simpler photometric searches in $griz$+WISE\n",
      "based on colour cuts would provide samples with $7\\%$ purity or less. Our\n",
      "technique is extremely fast, as a list of candidates can be obtained from a\n",
      "stage III experiment (e.g. DES catalog/database) in {a few} CPU hours.\n",
      "  The techniques are easily extendable to Stage IV experiments like LSST with\n",
      "the addition of time domain information.\n"
     ]
    }
   ],
   "source": [
    "j = np.random.randint(len(labels))\n",
    "print(j, target_name_dict[labels[j][0]])\n",
    "print(abstracts[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "maxlen = 150\n",
    "max_words = 10000 # Top 10000 words\n",
    "training_samples = 6000 #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 78157 unique tokens\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(abstracts)\n",
    "sequences = tokenizer.texts_to_sequences(abstracts)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print(\"Found %s unique tokens\" % len(word_index))\n",
    "\n",
    "word_index_reverse = dict()\n",
    "\n",
    "for k, v in word_index.items():\n",
    "    word_index_reverse[v] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pad_sequences(sequences=sequences, maxlen=maxlen)\n",
    "indices = np.arange(abstracts.shape[0])\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "data = data[indices]\n",
    "\n",
    "labels = labels[indices]\n",
    "\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "\n",
    "x_test = data[training_samples:]\n",
    "y_test = labels[training_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/41971587/how-to-convert-predicted-sequence-back-to-text-in-keras\n",
    "# Creating a reverse dictionary\n",
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "\n",
    "# Function takes a tokenized sentence and returns the words\n",
    "def sequence_to_text(list_of_indices):\n",
    "    # Looking up words in dictionary\n",
    "    words = [reverse_word_map.get(letter) for letter in list_of_indices]\n",
    "    return(words)\n",
    "\n",
    "# Creating texts \n",
    "my_texts = list(map(sequence_to_text, data))\n",
    "\n",
    "# alternative way\n",
    "my_texts_2 = tokenizer.sequences_to_texts(sequences=sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1221 0\n",
      "[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 'we', 'report', 'the', 'discovery', 'of', 'a', 'narrow', 'stellar', 'stream', 'crossing', 'the', 'of', 'sculptor', 'and', 'fornax', 'in', 'the', 'southern', 'celestial', 'hemisphere', 'the', 'portion', 'of', 'the', 'stream', 'detected', 'in', 'the', 'data', 'release', '1', 'photometry', 'of', 'the', 'atlas', 'survey', 'is', 'at', 'least', '12', 'degrees', 'long', 'while', 'its', 'width', 'is', 'approx', '0', '25', 'deg', 'the', 'color', 'magnitude', 'diagram', 'of', 'this', 'halo', 'sub', 'structure', 'is', 'consistent', 'with', 'a', 'metal', 'poor', 'fe', 'h', 'lesssim', '1', '4', 'stellar', 'population', 'located', 'at', 'a', 'heliocentric', 'distance', 'of', '20', 'pm', '2', 'kpc', 'there', 'are', 'three', 'globular', 'clusters', 'that', 'could', 'tentatively', 'be', 'associated', 'with', 'the', 'stream', 'ngc', 'ngc', 'm15', 'and', 'but', 'ngc', 'and', 'seem', 'to', 'have', 'proper', 'motions', 'incompatible', 'with', 'the', 'stream', 'orbit']\n"
     ]
    }
   ],
   "source": [
    "j = np.random.randint(len(x_train))\n",
    "print(j, target_name_dict[y_train[j][0]])\n",
    "print(my_texts[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['astro-ph.SR', 'astro-ph.EP', 'astro-ph.EP', ..., 'astro-ph.CO',\n",
       "       'astro-ph.GA', 'astro-ph.HE'], dtype='<U18')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = np.asarray([item[0] for item in y_train.tolist()])\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['astro-ph.SR', 'astro-ph.SR', 'astro-ph.CO', ..., 'astro-ph.HE',\n",
       "       'astro-ph', 'gr-qc'], dtype='<U18')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = np.asarray([item[0] for item in y_test.tolist()])\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['astro-ph', 'astro-ph.CO', 'astro-ph.EP', 'astro-ph.GA',\n",
       "       'astro-ph.HE', 'astro-ph.IM', 'astro-ph.SR', 'cond-mat.mtrl-sci',\n",
       "       'cond-mat.quant-gas', 'cond-mat.stat-mech', 'cs.CV', 'cs.DB',\n",
       "       'gr-qc', 'hep-ex', 'hep-ph', 'hep-th', 'math-ph', 'math.CA',\n",
       "       'math.NA', 'math.OC', 'nucl-ex', 'nucl-th', 'physics.ao-ph',\n",
       "       'physics.atom-ph', 'physics.chem-ph', 'physics.comp-ph',\n",
       "       'physics.data-an', 'physics.ed-ph', 'physics.flu-dyn',\n",
       "       'physics.gen-ph', 'physics.geo-ph', 'physics.hist-ph',\n",
       "       'physics.ins-det', 'physics.med-ph', 'physics.optics',\n",
       "       'physics.plasm-ph', 'physics.soc-ph', 'physics.space-ph',\n",
       "       'quant-ph'], dtype='<U18')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_labels = np.unique(y_train)\n",
    "unique_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_labels = ['astro-ph.GA', 'astro-ph.SR', 'astro-ph.IM', 'astro-ph.EP', 'astro-ph.HE', 'astro-ph.CO' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['astro-ph.SR', 'astro-ph.EP', 'astro-ph.EP', ..., 'astro-ph.CO',\n",
       "       'astro-ph.GA', 'astro-ph.HE'], dtype='<U11')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_selected = np.asarray([item for item in y_train.tolist() if item in selected_labels])\n",
    "y_train_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "jj = 0 \n",
    "x_train_selected = []\n",
    "\n",
    "for item in y_train.tolist(): \n",
    "    if item in selected_labels:\n",
    "        x_train_selected.append(x_train[jj,:])\n",
    "        jj = jj + 1\n",
    "        \n",
    "x_train_selected = np.asarray(x_train_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['astro-ph.SR', 'astro-ph.SR', 'astro-ph.CO', ..., 'astro-ph.CO',\n",
       "       'astro-ph.CO', 'astro-ph.HE'], dtype='<U11')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_selected = np.asarray([item for item in y_test.tolist() if item in selected_labels])\n",
    "y_test_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "jj = 0 \n",
    "x_test_selected = []\n",
    "\n",
    "for item in y_test.tolist(): \n",
    "    if item in selected_labels:\n",
    "        x_test_selected.append(x_test[jj,:])\n",
    "        jj = jj + 1\n",
    "        \n",
    "x_test_selected = np.asarray(x_test_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_num = np.asarray([target_name_dict[x] for x in y_train_selected.tolist()])\n",
    "y_test_num = np.asarray([target_name_dict[x] for x in y_test_selected.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "y_train_one_hot = to_categorical(y_train_num)\n",
    "y_test_one_hot = to_categorical(y_test_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "import keras.layers as layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/local/EC/andreas.merentitis/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 150, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 15000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                480032    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 198       \n",
      "=================================================================\n",
      "Total params: 1,481,286\n",
      "Trainable params: 1,481,286\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "embeddings_dim = 100\n",
    "model.add(layers.Embedding(max_words, embeddings_dim, input_length=maxlen))\n",
    "model.add(layers.Flatten())\n",
    "#model.add(layers.Dense(64, activation='relu', input_shape=(maxlen,)))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dropout(rate=0.3))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(6, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.layers[0].set_weights([embedding_matrix])\n",
    "#model.layers[0].trainable= False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/local/EC/andreas.merentitis/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 3608 samples, validate on 1547 samples\n",
      "Epoch 1/5\n",
      "3608/3608 [==============================] - 2s 505us/step - loss: 1.7092 - acc: 0.2447 - val_loss: 1.7068 - val_acc: 0.2540\n",
      "Epoch 2/5\n",
      "3608/3608 [==============================] - 2s 461us/step - loss: 1.4598 - acc: 0.4360 - val_loss: 1.7567 - val_acc: 0.2372\n",
      "Epoch 3/5\n",
      "3608/3608 [==============================] - 2s 485us/step - loss: 0.7994 - acc: 0.7336 - val_loss: 2.0330 - val_acc: 0.2224\n",
      "Epoch 4/5\n",
      "3608/3608 [==============================] - 2s 506us/step - loss: 0.2643 - acc: 0.9465 - val_loss: 2.6536 - val_acc: 0.2379\n",
      "Epoch 5/5\n",
      "3608/3608 [==============================] - 2s 483us/step - loss: 0.0777 - acc: 0.9856 - val_loss: 3.0783 - val_acc: 0.2120\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train_selected, y_train_one_hot,\n",
    "                   epochs=5,\n",
    "                   batch_size=32,\n",
    "                   validation_split=0.3)\n",
    "#model.save_weights('pre_trained_glove_model_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46473/46473 [==============================] - 2s 39us/step\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(x_test_selected, y_test_one_hot)\n",
    "class_prediction = model.predict_classes(x_test_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.022110013562863, 0.23108041286468506]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1 4740  182   48    3  603 4414    1  382  633   15 3972    3  603\n",
      " 4414   12 2381 1354  220    3 2201 1354  220  424    7   45   10    4\n",
      "  175  159 2143 8024   15   96 2747   11   78   41 1042    2    1   70\n",
      "    3    1  159 2143 1048 6165 1108   41  557 1751  983   22  927  876\n",
      "  619    1  159 1024    8  249   15  213   41  207    1  382  291  801\n",
      "  143   21 4414   75   15 5096 8290    7   45   10    1  159 2143    8\n",
      " 1460  165    1  291  801    8  213    3    1  159 1024 1206 4998   15\n",
      "    4  850  291  801   11  285 8290    1  175  159 2143    3   67  291\n",
      "  801   60    5    4 2535    3  876 1172 1256    9  432  775  631  437\n",
      "   54  753   10 5577    4  467   20   22 1483 1990   11 1452   42   96\n",
      "    1   38   14    1 4289 4210   12  434   11 7921]\n",
      "['the', 'snow', 'surface', 'between', 'and', '14', '5m', 'the', 'average', 'temperatures', 'at', '2m', 'and', '14', '5m', 'are', '54', 'circ', 'c', 'and', '46', 'circ', 'c', 'respectively', 'we', 'find', 'that', 'a', 'strong', 'temperature', 'inversion', 'existed', 'at', 'all', 'heights', 'for', 'more', 'than', '70', 'of', 'the', 'time', 'and', 'the', 'temperature', 'inversion', 'typically', 'lasts', 'longer', 'than', '25', 'hours', 'indicating', 'an', 'extremely', 'stable', 'atmosphere', 'the', 'temperature', 'gradient', 'is', 'larger', 'at', 'lower', 'than', 'higher', 'the', 'average', 'wind', 'speed', 'was', '1', '5m', 's', 'at', '4m', 'elevation', 'we', 'find', 'that', 'the', 'temperature', 'inversion', 'is', 'stronger', 'when', 'the', 'wind', 'speed', 'is', 'lower', 'and', 'the', 'temperature', 'gradient', 'decreases', 'sharply', 'at', 'a', 'specific', 'wind', 'speed', 'for', 'each', 'elevation', 'the', 'strong', 'temperature', 'inversion', 'and', 'low', 'wind', 'speed', 'results', 'in', 'a', 'shallow', 'and', 'stable', 'boundary', 'layer', 'with', 'weak', 'atmospheric', 'turbulence', 'above', 'it', 'suggesting', 'that', 'dome', 'a', 'should', 'be', 'an', 'excellent', 'site', 'for', 'astronomical', 'observations', 'all', 'the', 'data', 'from', 'the', 'weather', 'station', 'are', 'available', 'for', 'download']\n",
      "true class:  5\n",
      "prediction:  5\n"
     ]
    }
   ],
   "source": [
    "jj = np.random.randint(len(x_test_selected))\n",
    "print(x_test_selected[jj])\n",
    "print(my_texts[training_samples+jj])\n",
    "print(\"true class: \", target_name_dict[y_test_selected[jj]])\n",
    "print(\"prediction: \", class_prediction[jj])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0   17    8    4   85 2232  133]]\n",
      "prediction:  [0.05442783 0.31576627 0.0355003  0.04861952 0.3085935  0.23709252]\n",
      "predicted category:  astro-ph.SR\n"
     ]
    }
   ],
   "source": [
    "abstract_testing = \"this is a new extrasolar system\"\n",
    "seq_testing = tokenizer.texts_to_sequences([[ w for w in abstract_testing.split(' ')]])\n",
    "data_testing = pad_sequences(sequences=seq_testing, maxlen=maxlen)\n",
    "print(data_testing)\n",
    "classes_testing = model.predict(data_testing)\n",
    "print(\"prediction: \", classes_testing[0])\n",
    "print(\"predicted category: \", label2target[np.argmax(classes_testing[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
