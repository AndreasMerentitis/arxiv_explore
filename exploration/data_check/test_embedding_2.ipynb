{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "target_name_dict = { 'astro-ph.GA' : 0,\n",
    "                    'astro-ph.SR' : 1,\n",
    "                    'astro-ph.IM' : 2,\n",
    "                    'astro-ph.EP' : 3,\n",
    "                    'astro-ph.HE' : 4,\n",
    "                    'astro-ph.CO' : 5\n",
    "                }\n",
    "label2target = { v:k for k,v in target_name_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.HDFStore(\"../data/2014astroph_p.h5\", \"r\")\n",
    "df['/df'].keys()\n",
    "abstracts = df['/df']['abstract']\n",
    "labels = np.array(df['/df']['label'])\n",
    "df.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8489 astro-ph.HE\n",
      "I review our current state of knowledge about non-thermal radiation from the\n",
      "Galactic Centre (GC) and Inner Galaxy. Definitionally, the Galactic nucleus is\n",
      "at the bottom of the Galaxy's gravitational well, rendering it a promising\n",
      "region to seek the signatures of dark matter decay or annihilation. It also\n",
      "hosts, however, the Milky Way's resident supermassive black hole and up to 10%\n",
      "of current massive star formation in the Galaxy. Thus the Galactic nucleus is a\n",
      "dynamic and highly-energized environment implying that extreme caution must be\n",
      "exercised in interpreting any unusual or unexpected signal from (or emerging\n",
      "from) the region as evidence for dark matter-related processes. One spectacular\n",
      "example of an `unexpected' signal is the discovery within the last few years of\n",
      "the `Fermi Bubbles' and, subsequently, their polarised radio counterparts.\n",
      "These giant lobes extend ~7 kpc from the nucleus into both north and south\n",
      "Galactic hemispheres. Hard-spectrum, microwave emission coincident with the\n",
      "lower reaches of the Bubbles has also been detected, first in WMAP, and more\n",
      "recently in Planck data. Debate continues as to the origin of the Bubbles and\n",
      "their multi-wavelength emissions: are they the signatures of relatively recent\n",
      "(in the last ~Myr) activity of the supermassive black hole or, alternatively,\n",
      "nuclear star formation? I will briefly review evidence that points to the\n",
      "latter interpretation.\n"
     ]
    }
   ],
   "source": [
    "j = np.random.randint(len(labels))\n",
    "print(j, label2target[labels[j]])\n",
    "print(abstracts[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "maxlen = 150\n",
    "max_words = 10000 # Top 10000 words\n",
    "training_samples = 6000 #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30677 unique tokens\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(abstracts)\n",
    "sequences = tokenizer.texts_to_sequences(abstracts)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print(\"Found %s unique tokens\" % len(word_index))\n",
    "\n",
    "word_index_reverse = dict()\n",
    "\n",
    "for k, v in word_index.items():\n",
    "    word_index_reverse[v] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pad_sequences(sequences=sequences, maxlen=maxlen)\n",
    "indices = np.arange(abstracts.shape[0])\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "data = data[indices]\n",
    "\n",
    "labels = labels[indices]\n",
    "\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "\n",
    "x_test = data[training_samples:]\n",
    "y_test = labels[training_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/41971587/how-to-convert-predicted-sequence-back-to-text-in-keras\n",
    "# Creating a reverse dictionary\n",
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "\n",
    "# Function takes a tokenized sentence and returns the words\n",
    "def sequence_to_text(list_of_indices):\n",
    "    # Looking up words in dictionary\n",
    "    words = [reverse_word_map.get(letter) for letter in list_of_indices]\n",
    "    return(words)\n",
    "\n",
    "# Creating texts \n",
    "my_texts = list(map(sequence_to_text, data))\n",
    "\n",
    "# alternative way\n",
    "my_texts_2 = tokenizer.sequences_to_texts(sequences=sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2788 astro-ph.CO\n",
      "['ia', 'data', 'in', 'recent', 'years', 'we', 'use', 'the', 'union2', '1', 'data', 'to', 'give', 'a', 'simple', 'classification', 'of', 'such', 'studies', 'for', 'the', 'first', 'time', 'because', 'the', 'maximum', 'anisotropic', 'direction', 'is', 'independent', 'of', 'isotropic', 'dark', 'energy', 'models', 'we', 'adopt', 'two', 'cosmological', 'models', 'lambda', 'cdm', 'w', 'cdm', 'for', 'the', 'hemisphere', 'comparison', 'analysis', 'and', 'lambda', 'cdm', 'model', 'for', 'dipole', 'fit', 'approach', 'in', 'hemisphere', 'comparison', 'method', 'the', 'matter', 'density', 'and', 'the', 'equation', 'of', 'state', 'of', 'dark', 'energy', 'are', 'adopted', 'as', 'the', 'diagnostic', 'in', 'the', 'lambda', 'cdm', 'model', 'and', 'w', 'cdm', 'model', 'respectively', 'in', 'dipole', 'fit', 'approach', 'we', 'fit', 'the', 'fluctuation', 'of', 'distance', 'modulus', 'we', 'find', 'that', 'there', 'is', 'a', 'null', 'signal', 'for', 'the', 'hemisphere', 'comparison', 'method', 'while', 'a', 'preferred', 'direction', 'b', '14', '3', 'circ', 'pm', '10', '1', 'circ', 'l', '1', 'circ', 'pm', '16', '2', 'circ', 'for', 'the', 'dipole', 'fit', 'method', 'this', 'result', 'indicates', 'that', 'the', 'dipole', 'fit', 'is', 'more', 'sensitive', 'than', 'the', 'hemisphere', 'comparison', 'method']\n"
     ]
    }
   ],
   "source": [
    "j = np.random.randint(len(x_train))\n",
    "print(j, label2target[y_train[j]])\n",
    "print(my_texts[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "y_train_one_hot = to_categorical(y_train)\n",
    "y_test_one_hot = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "import keras.layers as layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 150, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 15000)             0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 32)                480032    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 6)                 198       \n",
      "=================================================================\n",
      "Total params: 1,481,286\n",
      "Trainable params: 1,481,286\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "embeddings_dim = 100\n",
    "model.add(layers.Embedding(max_words, embeddings_dim, input_length=maxlen))\n",
    "model.add(layers.Flatten())\n",
    "#model.add(layers.Dense(64, activation='relu', input_shape=(maxlen,)))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dropout(rate=0.3))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(6, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.layers[0].set_weights([embedding_matrix])\n",
    "#model.layers[0].trainable= False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4200 samples, validate on 1800 samples\n",
      "Epoch 1/5\n",
      "4200/4200 [==============================] - 1s 147us/step - loss: 1.6003 - acc: 0.3450 - val_loss: 1.3847 - val_acc: 0.3994\n",
      "Epoch 2/5\n",
      "4200/4200 [==============================] - 0s 87us/step - loss: 1.0412 - acc: 0.6081 - val_loss: 1.1023 - val_acc: 0.5794\n",
      "Epoch 3/5\n",
      "4200/4200 [==============================] - 0s 89us/step - loss: 0.4507 - acc: 0.8712 - val_loss: 1.0318 - val_acc: 0.6289\n",
      "Epoch 4/5\n",
      "4200/4200 [==============================] - 0s 84us/step - loss: 0.1247 - acc: 0.9714 - val_loss: 1.1760 - val_acc: 0.6483\n",
      "Epoch 5/5\n",
      "4200/4200 [==============================] - 0s 83us/step - loss: 0.0387 - acc: 0.9936 - val_loss: 1.3956 - val_acc: 0.6422\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train, y_train_one_hot,\n",
    "                   epochs=5,\n",
    "                   batch_size=32,\n",
    "                   validation_split=0.3)\n",
    "#model.save_weights('pre_trained_glove_model_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2794/2794 [==============================] - 0s 35us/step\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(x_test, y_test_one_hot)\n",
    "class_prediction = model.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.4146753831172552, 0.641374373615172]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 501  263  220   93    3 1124 2322    2   76   66    3  373   77    5\n",
      "    1  111  786  532  318  142   65  990    8 1555   13 5715 6251 1001\n",
      "  549  799    5    1   36  391  186 1294  746   13    4  337    2   25\n",
      "    5   30  254   26    1 1012  117  686 1000 4167    3    4  705  460\n",
      "  145 1995  111  140 4182  901    4   82 1091  224 2030    2   77   24\n",
      "    6   33  307   13    4   65  278   24  100 2562 1260  250  269  746\n",
      "    1  655    2    1  990   34 2026 1856 1006    3 5963 2369 2164 1771\n",
      "  146  990    8 1157  193   77  646   13  128 1006   23   12 5362    6\n",
      "  697    5   76   24   39 2369  852  835 1000 3716    1  123 1112   52\n",
      "  301 7274    1  873 2266   12  241    3    1  765 2369  429   12  423\n",
      "  481   46 2317 1160  269   10  572   20   79   22]\n",
      "['curves', 'v', 'c', 'r', 'and', 'greater', 'amounts', 'of', 'low', 'density', 'and', 'hot', 'gas', 'in', 'the', 'disk', 'mid', 'plane', 'ii', 'when', 'stellar', 'feedback', 'is', 'modeled', 'by', 'temporarily', 'switching', 'off', 'radiative', 'cooling', 'in', 'the', 'star', 'forming', 'regions', 're', 'increases', 'by', 'a', 'factor', 'of', '2', 'in', 'our', 'particular', 'model', 'the', 'circular', 'velocity', 'curve', 'becomes', 'flatter', 'and', 'a', 'complex', 'multi', 'phase', 'gaseous', 'disk', 'structure', 'develops', 'iii', 'a', 'more', 'efficient', 'local', 'conversion', 'of', 'gas', 'mass', 'to', 'stars', 'measured', 'by', 'a', 'stellar', 'particle', 'mass', 'distribution', 'biased', 'toward', 'larger', 'values', 'increases', 'the', 'strength', 'of', 'the', 'feedback', 'energy', 'injection', 'driving', 'outflows', 'and', 'inducing', 'sf', 'histories', 'iv', 'if', 'feedback', 'is', 'too', 'strong', 'gas', 'loss', 'by', 'galactic', 'outflows', 'which', 'are', 'easier', 'to', 'produce', 'in', 'low', 'mass', 'galaxies', 'sf', 'whose', 'history', 'becomes', 'episodic', 'the', 'simulations', 'exhibit', 'two', 'important', 'shortcomings', 'the', 'baryon', 'fractions', 'are', 'higher', 'and', 'the', 'specific', 'sf', 'rates', 'are', 'much', 'smaller', 'than', 'observationally', 'inferred', 'values', 'for', 'redshifts', '0', '4', '1']\n",
      "astro-ph.CO\n",
      "prediction:  astro-ph.GA\n"
     ]
    }
   ],
   "source": [
    "jj = np.random.randint(len(x_test))\n",
    "print(x_test[jj])\n",
    "print(my_texts[training_samples+jj])\n",
    "print(label2target[y_test[jj]])\n",
    "print(\"prediction: \", label2target[class_prediction[jj]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0   16    8    4   75 2312  118]]\n",
      "prediction:  [0.18530409 0.6776955  0.04168797 0.02523528 0.03012412 0.039953  ]\n",
      "predicted category:  astro-ph.SR\n"
     ]
    }
   ],
   "source": [
    "abstract_testing = \"this is a new extrasolar system\"\n",
    "seq_testing = tokenizer.texts_to_sequences([[ w for w in abstract_testing.split(' ')]])\n",
    "data_testing = pad_sequences(sequences=seq_testing, maxlen=maxlen)\n",
    "print(data_testing)\n",
    "classes_testing = model.predict(data_testing)\n",
    "print(\"prediction: \", classes_testing[0])\n",
    "print(\"predicted category: \", label2target[np.argmax(classes_testing[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu-2",
   "language": "python",
   "name": "tf-gpu-2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
