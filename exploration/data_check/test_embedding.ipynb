{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "target_name_dict = { 'astro-ph.GA' : 0,\n",
    "                    'astro-ph.SR' : 1,\n",
    "                    'astro-ph.IM' : 2,\n",
    "                    'astro-ph.EP' : 3,\n",
    "                    'astro-ph.HE' : 4,\n",
    "                    'astro-ph.CO' : 5\n",
    "                }\n",
    "label2target = { v:k for k,v in target_name_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.HDFStore(\"../data/2014astroph_p.h5\", \"r\")\n",
    "df['/df'].keys()\n",
    "abstracts = df['/df']['abstract']\n",
    "labels = np.array(df['/df']['label'])\n",
    "df.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6721 astro-ph.IM\n",
      "Searching for nearby habitable worlds with direct imaging and spectroscopy\n",
      "will require a telescope large enough to provide angular resolution and\n",
      "sensitivity to planets around a significant sample of stars. Segmented\n",
      "telescopes are a compelling option to obtain such large apertures. However,\n",
      "these telescope designs have a complex geometry (central obstruction, support\n",
      "structures, segmentation) that makes high-contrast imaging more challenging. We\n",
      "are developing a new high-contrast imaging testbed at STScI to provide an\n",
      "integrated solution for wavefront control and starlight suppression on complex\n",
      "aperture geometries. We present our approach for the testbed optical design,\n",
      "which defines the surface requirements for each mirror to minimize the\n",
      "amplitude-induced errors from the propagation of out-of-pupil surfaces. Our\n",
      "approach guarantees that the testbed will not be limited by these Fresnel\n",
      "propagation effects, but only by the aperture geometry. This approach involves\n",
      "iterations between classical ray-tracing optical design optimization, and\n",
      "end-to-end Fresnel propagation with wavefront control (e.g. Electric Field\n",
      "Conjugation / Stroke Minimization). The construction of the testbed is planned\n",
      "to start in late Fall 2013.\n"
     ]
    }
   ],
   "source": [
    "j = np.random.randint(len(labels))\n",
    "print(j, label2target[labels[j]])\n",
    "print(abstracts[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "maxlen = 150\n",
    "max_words = 10000 # Top 10000 words\n",
    "training_samples = 6000 #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30677 unique tokens\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(abstracts)\n",
    "sequences = tokenizer.texts_to_sequences(abstracts)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print(\"Found %s unique tokens\" % len(word_index))\n",
    "\n",
    "word_index_reverse = dict()\n",
    "\n",
    "for k, v in word_index.items():\n",
    "    word_index_reverse[v] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pad_sequences(sequences=sequences, maxlen=maxlen)\n",
    "indices = np.arange(abstracts.shape[0])\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "data = data[indices]\n",
    "\n",
    "labels = labels[indices]\n",
    "\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "\n",
    "x_test = data[training_samples:]\n",
    "y_test = labels[training_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/41971587/how-to-convert-predicted-sequence-back-to-text-in-keras\n",
    "# Creating a reverse dictionary\n",
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "\n",
    "# Function takes a tokenized sentence and returns the words\n",
    "def sequence_to_text(list_of_indices):\n",
    "    # Looking up words in dictionary\n",
    "    words = [reverse_word_map.get(letter) for letter in list_of_indices]\n",
    "    return(words)\n",
    "\n",
    "# Creating texts \n",
    "my_texts = list(map(sequence_to_text, data))\n",
    "\n",
    "# alternative way\n",
    "my_texts_2 = tokenizer.sequences_to_texts(sequences=sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3824 astro-ph.CO\n",
      "['dark', 'matter', 'voids', 'et', 'al', '2013', 'concentrate', 'on', 'the', 'velocity', 'profiles', 'around', 'voids', 'first', 'they', 'show', 'the', 'necessity', 'of', 'four', 'parameters', 'to', 'describe', 'the', 'density', 'profiles', 'around', 'voids', 'given', 'two', 'distinct', 'void', 'populations', 'voids', 'in', 'voids', 'and', 'voids', 'in', 'clouds', 'this', 'profile', 'is', 'used', 'to', 'predict', 'peculiar', 'velocities', 'around', 'voids', 'and', 'the', 'combination', 'of', 'the', 'latter', 'with', 'void', 'density', 'profiles', 'allows', 'the', 'construction', 'of', 'model', 'void', 'galaxy', 'cross', 'correlation', 'functions', 'with', 'redshift', 'space', 'distortions', 'when', 'these', 'models', 'are', 'tuned', 'to', 'fit', 'the', 'measured', 'correlation', 'functions', 'for', 'voids', 'and', 'galaxies', 'in', 'the', 'sloan', 'digital', 'sky', 'survey', 'small', 'voids', 'are', 'found', 'to', 'be', 'of', 'the', 'void', 'in', 'cloud', 'type', 'whereas', 'larger', 'ones', 'are', 'consistent', 'with', 'being', 'void', 'in', 'void', 'this', 'is', 'a', 'novel', 'result', 'that', 'is', 'obtained', 'directly', 'from', 'redshift', 'space', 'data', 'around', 'voids', 'these', 'profiles', 'can', 'be', 'used', 'to', 'remove', 'systematics', 'on', 'void', 'galaxy', 'alcock', 'tests', 'coming', 'from', 'redshift', 'space', 'distortions']\n"
     ]
    }
   ],
   "source": [
    "j = np.random.randint(len(x_train))\n",
    "print(j, label2target[y_train[j]])\n",
    "print(my_texts[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "y_train_one_hot = to_categorical(y_train)\n",
    "y_test_one_hot = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing GloVe file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "glove_dir = \"../glove.6B/\"\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dim = 100 # same dimension as the glove.6B above\n",
    "embedding_vector = None\n",
    "embedding_matrix = np.zeros((max_words, embeddings_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "import keras.layers as layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 150, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 15000)             0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 64)                960064    \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 1,964,614\n",
      "Trainable params: 1,964,614\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "embeddings_dim = 100\n",
    "model.add(layers.Embedding(max_words, embeddings_dim, input_length=maxlen))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(maxlen,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(6, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.layers[0].set_weights([embedding_matrix])\n",
    "#model.layers[0].trainable= False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4200 samples, validate on 1800 samples\n",
      "Epoch 1/5\n",
      "4200/4200 [==============================] - 1s 196us/step - loss: 0.0356 - acc: 0.9914 - val_loss: 1.3419 - val_acc: 0.6100\n",
      "Epoch 2/5\n",
      "4200/4200 [==============================] - 0s 91us/step - loss: 0.0026 - acc: 0.9993 - val_loss: 1.6972 - val_acc: 0.5872\n",
      "Epoch 3/5\n",
      "4200/4200 [==============================] - 0s 97us/step - loss: 9.9830e-04 - acc: 0.9998 - val_loss: 1.7258 - val_acc: 0.6128\n",
      "Epoch 4/5\n",
      "4200/4200 [==============================] - 0s 94us/step - loss: 2.3619e-05 - acc: 1.0000 - val_loss: 2.1874 - val_acc: 0.6183\n",
      "Epoch 5/5\n",
      "4200/4200 [==============================] - 0s 90us/step - loss: 1.7071e-06 - acc: 1.0000 - val_loss: 2.2456 - val_acc: 0.5922\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train, y_train_one_hot,\n",
    "                   epochs=5,\n",
    "                   batch_size=32,\n",
    "                   validation_split=0.3)\n",
    "#model.save_weights('pre_trained_glove_model_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2794/2794 [==============================] - 0s 34us/step\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(x_test, y_test_one_hot)\n",
    "class_prediction = model.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.010026819209329, 0.6385110952466747]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  14    1 3667   29   73   27  164    1  873 1077 1038 1449    3    1\n",
      "  548  156  253   14    1  152  124 2233 5387    6  399    1   47   34\n",
      "   66    3  336    2  215    1  386   78 1410   27   19  997  575   13\n",
      " 4183   10   80  571  785    5    1  104   98   50   13   40    1 1180\n",
      "  200    7  382  148  509    1 4942  647 4484   23    8 1017  516 1963\n",
      "   21 6261    2    1 4484  647    9   82 4278 2216    3    1  486   80\n",
      " 4942  647   40    1  486   80  711    7 2138  243  591   20  244   20\n",
      "    3  243  591    4  244   20  748   14    1 1449    3 5387  183  243\n",
      "  207  249 1651  207  224  244  699   14    1  104   98  183    9   80\n",
      "  571  785    3  243  207  249 1651  207  224  244  988    3  243  207\n",
      "  249 1651  207 8352  244 7868   14    1 1180  183]\n",
      "['from', 'the', 'lss', 'data', 'one', 'can', 'use', 'the', 'baryon', 'acoustic', 'oscillation', 'bao', 'and', 'the', 'growth', 'rate', 'derived', 'from', 'the', 'redshift', 'space', 'distortion', 'rsd', 'to', 'measure', 'the', 'dark', 'energy', 'density', 'and', 'equation', 'of', 'state', 'the', 'primordial', 'non', 'gaussianity', 'can', 'be', 'constrained', 'either', 'by', 'looking', 'for', 'scale', 'dependent', 'bias', 'in', 'the', 'power', 'spectrum', 'or', 'by', 'using', 'the', 'bispectrum', 'here', 'we', 'consider', 'three', 'cases', 'the', 'cylinder', 'array', 'pathfinder', 'which', 'is', 'currently', 'being', 'built', 'an', 'upgrade', 'of', 'the', 'pathfinder', 'array', 'with', 'more', 'receiver', 'units', 'and', 'the', 'full', 'scale', 'cylinder', 'array', 'using', 'the', 'full', 'scale', 'experiment', 'we', 'expect', 'sigma', 'w', '0', 'sim', '0', 'and', 'sigma', 'w', 'a', 'sim', '0', '21', 'from', 'the', 'bao', 'and', 'rsd', 'measurements', 'sigma', 'rm', 'f', 'nl', 'rm', 'local', 'sim', '14', 'from', 'the', 'power', 'spectrum', 'measurements', 'with', 'scale', 'dependent', 'bias', 'and', 'sigma', 'rm', 'f', 'nl', 'rm', 'local', 'sim', '22', 'and', 'sigma', 'rm', 'f', 'nl', 'rm', 'equil', 'sim', '157', 'from', 'the', 'bispectrum', 'measurements']\n",
      "astro-ph.CO\n",
      "prediction:  astro-ph.CO\n"
     ]
    }
   ],
   "source": [
    "jj = np.random.randint(len(x_test))\n",
    "print(x_test[jj])\n",
    "print(my_texts[training_samples+jj])\n",
    "print(label2target[y_test[jj]])\n",
    "print(\"prediction: \", label2target[class_prediction[jj]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    1   91   63  581  578   71  424  981   11    1  578    8  557\n",
      "  3258    9   21  861  189    2   17  642    1 1550    2   70 3258 2101\n",
      "     5    1   86  118    1   91   63  581  578   71  424  981   11    1\n",
      "   578    8  557 3258    9   21  861  189    2   17  642    1 1550    2\n",
      "    70 3258 2101    5    1   86  118    1   91   63  581  578   71  424\n",
      "   981   11    1  578    8  557 3258    9   21  861  189    2   17  642\n",
      "     1 1550    2   70 3258 2101    5    1   86  118]]\n",
      "prediction:  [1.9235373e-05 1.3179114e-02 4.2389169e-05 9.8675913e-01 2.9308373e-07\n",
      " 1.0821752e-11]\n",
      "predicted category:  astro-ph.EP\n"
     ]
    }
   ],
   "source": [
    "abstract_testing = \"the first observed interstellar object Its light-curve amplitude indicates that the object is highly elongated with an axis ratio of at least 5:1. the absence of such elongated asteroids in the Solar system the first observed interstellar object Its light-curve amplitude indicates that the object is highly elongated with an axis ratio of at least 5:1. the absence of such elongated asteroids in the Solar system the first observed interstellar object Its light-curve amplitude indicates that the object is highly elongated with an axis ratio of at least 5:1. the absence of such elongated asteroids in the Solar system\"\n",
    "\n",
    "seq_testing = tokenizer.texts_to_sequences([[ w for w in abstract_testing.split(' ')]])\n",
    "data_testing = pad_sequences(sequences=seq_testing, maxlen=maxlen)\n",
    "print(data_testing)\n",
    "classes_testing = model.predict(data_testing)\n",
    "print(\"prediction: \", classes_testing[0])\n",
    "print(\"predicted category: \", label2target[np.argmax(classes_testing[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu-2",
   "language": "python",
   "name": "tf-gpu-2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
